Systems Architecture for Vision-Centric Autonomy: Reconfiguring the UGV Beast for RGB-D Navigation and Mapping1. Introduction: The Paradigm Shift from LiDAR to Vision-Based SLAMThe evolution of autonomous mobile robotics has historically been bifurcated into two distinct sensory lineages: laser-based perception and vision-based perception. The UGV Beast platform, in its standard configuration, typically relies on a hybrid approach, utilizing a 2D LiDAR (Light Detection and Ranging) for metric accuracy in simultaneous localization and mapping (SLAM), and an RGB-D camera for depth sensing and object recognition. However, operational requirements often necessitate the removal of LiDAR dependencies—whether for cost reduction, stealth (removing active laser emissions), mechanical simplicity, or operation in environments where planar 2D scanning provides insufficient data density. This research report details the architectural and configuration overhaul required to transition the UGV Beast to a pure RGB-D autonomy stack, leveraging the OAK-D Pro active stereo camera, RTAB-Map (Real-Time Appearance-Based Mapping), and the ROS 2 Navigation Stack (Nav2).The transition from LiDAR to Visual SLAM (VSLAM) is not merely a substitution of sensors; it represents a fundamental shift in how the robot represents spatial occupancy. LiDAR provides deterministic, high-precision range data on a single plane, making grid-based mapping computationally trivial but informationally sparse. Conversely, RGB-D sensors provide volumetric, dense, but stochastically noisy data that requires sophisticated filtering, projection, and probabilistic integration to generate navigable costmaps. The successful reconfiguration of the UGV Beast necessitates a deep understanding of three interconnected domains: the proprietary low-level control interface of the ESP32 sub-controller 1, the active stereo mechanics of the Luxonis OAK-D Pro 2, and the graph-optimization parameters within RTAB-Map.4This report analyzes the complete system pipeline, from the raw JSON command packets governing motor actuation to the high-level behavior trees orchestrating autonomous exploration. It integrates findings from the Sasquatch Robotics architecture 1, the Waveshare UGV documentation 1, and the official Nav2 and RTAB-Map repositories to provide a robust blueprint for vision-only autonomy.2. Hardware Abstraction and Sub-Controller InterfaceThe foundation of the UGV Beast’s autonomy lies in its tiered control architecture. While high-level planning occurs on the host computer (typically a Jetson Orin Nano or Raspberry Pi), real-time actuation is offloaded to an ESP32 microcontroller. Understanding the communication protocol between these layers is critical for ensuring that the vision-based navigation commands are accurately translated into physical motion.2.1 The JSON Command ProtocolUnlike many ROS-native platforms that utilize binary protocols over serial (like rosserial), the UGV Beast employs a human-readable, text-based JSON command set over UART.1 This design choice prioritizes interoperability but introduces serialization overhead that the ROS 2 driver node must manage efficiently.The primary interface for motion control is not a direct PWM signal but a velocity request. The Nav2 stack’s local planner (e.g., DWB Controller) publishes geometry_msgs/Twist messages containing linear ($v_x$) and angular ($\omega_z$) velocity targets. The hardware interface node (often ugv_driver or slushe_driver_node) must subscribe to these messages and serialize them into the CMD_ROS_CTRL format.Motion Command Structure:ParameterJSON KeyValue DescriptionUnitCommand TypeT13 (CMD_ROS_CTRL)Integer IDLinear VelocityXTarget forward speed$m/s$Angular VelocityZTarget rotation speed$rad/s$The resulting packet transmitted to the ESP32 appears as: {"T":13,"X":0.25,"Z":-0.1}. The firmware on the sub-controller receives this packet, parses the target velocities, and executes a closed-loop PID control algorithm to adjust the left and right wheel speeds accordingly.1 This abstraction shields the ROS 2 host from the complexities of differential drive kinematics, allowing the navigation stack to operate in the robot's base frame (base_link) without managing individual wheel encoders directly.2.2 Chassis Feedback and OdometryIn a LiDAR-less system, accurate wheel odometry becomes critically important as a "prior" or "guess" for the Visual SLAM system. Visual odometry can suffer from scale drift or tracking loss in texture-less environments. Fusing robust wheel encoder data stabilizes the visual solution.The ESP32 provides a continuous stream of chassis status data, which must be explicitly enabled during the driver node's initialization sequence. The command CMD_BASE_FEEDBACK_FLOW (T:131) with the argument cmd:1 activates this telemetry stream.1Feedback Stream Configuration:Command NameType (T)ArgumentFunctionCMD_BASE_FEEDBACK_FLOW131cmd:1Enables continuous data stream.CMD_FEEDBACK_FLOW_INTERVAL142cmd:10Sets interval to 10ms (100Hz).The default feedback interval may be insufficient for high-speed visual tracking. It is recommended to utilize CMD_FEEDBACK_FLOW_INTERVAL (T:142) to increase the frequency of feedback packets (e.g., to 50Hz or 100Hz), ensuring that the timestamped odometry messages published to ROS 2 are tightly synchronized with the camera frames. The driver node must parse the incoming JSON, extract encoder ticks or calculated velocities, integrate them over time to compute the robot's pose ($x, y, \theta$), and publish this to the /odom topic while broadcasting the odom -> base_link TF transform.2.3 Safety Mechanisms: The HeartbeatRemoving the LiDAR eliminates a primary safety sensor often used for hardware-level emergency stopping. Therefore, software-level safety becomes paramount. The UGV Beast protocol includes a heartbeat mechanism, CMD_HEART_BEAT_SET (T:136), which sets a timeout interval.1 If the sub-controller does not receive a valid motion command within this window (e.g., 500ms), it automatically halts all motors. The ROS 2 driver must be configured to send this heartbeat configuration on startup and maintain a steady stream of commands (even zero-velocity commands) to prevent unintended disarming during autonomous navigation.3. The Perception Stack: Optimizing OAK-D Pro for Active StereoThe removal of the D500 LiDAR shifts the burden of metric depth estimation entirely to the OAK-D Pro camera. While passive stereo cameras rely on ambient light and natural texture to calculate disparity, the OAK-D Pro features an active IR dot projector. This component is the linchpin of the LiDAR-less architecture, enabling the robot to perceive depth on featureless surfaces like white walls—a scenario where passive stereo (and thus standard Visual SLAM) typically fails.3.1 Active Stereo ConfigurationThe depthai_ros_driver in ROS 2 Humble exposes a comprehensive set of parameters to control the camera's internal processing pipeline. For mapping purposes, the configuration must prioritize the density and accuracy of the point cloud over the aesthetic quality of the RGB image.Critical Active Stereo Parameters 2:Parameter NamespaceParameterValueRationalecamerai_pipeline_typeRGBDEnsures synchronized RGB and Depth output.camerai_enable_irtrueActivates the IR subsystem.camerai_laser_dot_brightness800Sets projector intensity (0-1200). 800mA is optimal for indoor range without overheating.camerai_floodlight_brightness0Disables floodlight to prevent washing out the high-contrast dot pattern.The i_laser_dot_brightness parameter is crucial. The projector overlays a pseudo-random speckle pattern onto the scene. The stereo matching engine on the OAK-D's Myriad X VPU uses this pattern to establish correspondence between left and right images. A value of 800 (representing milliamperes) provides sufficient contrast for indoor mapping up to 5-7 meters.3 Setting this to 0 effectively reverts the camera to passive mode, rendering it useless for mapping blank corridors. Conversely, the i_floodlight_brightness should generally be kept at 0 unless the environment is in total darkness, as the floodlight reduces the contrast of the dot pattern, degrading depth accuracy.3.2 Depth-RGB Alignment and Point Cloud GenerationFor RTAB-Map to texture the 3D map correctly and for the Nav2 stack to identify obstacles, the depth map must be perfectly registered to the RGB camera's optical frame.Stereo Pipeline Configuration 2:YAMLstereo:
  i_align_depth: true
  i_subpixel: true
  i_lr_check: true
  i_extended_disp: false
  i_depth_filter_size: 5
i_align_depth: true: This forces the VPU to warp the depth map to match the perspective of the RGB camera. This results in a sensor_msgs/PointCloud2 where the color of each point corresponds exactly to the visual texture, essential for loop closure detection in RTAB-Map.i_subpixel: true: Enables sub-pixel disparity interpolation, increasing the quantization levels of depth z-coordinates. This reduces the "layering" effect seen in point clouds, providing a smoother surface for the SLAM algorithm to match against.i_lr_check: true: Performs a Left-Right consistency check, discarding pixels where the disparity computed from left-to-right does not match right-to-left. This effectively removes "ghost" points at object borders, which is critical for preventing the robot from seeing obstacles that don't exist (false positives in the costmap).3.3 Bandwidth Management and DDS TuningA raw PointCloud2 stream at 30Hz can easily saturate the bandwidth of the internal bus or the DDS (Data Distribution Service) middleware, causing latency that destabilizes the control loop. In a mapping configuration, high framerate is less critical than data integrity.It is recommended to reduce the publishing rate of the point cloud to 10-15Hz using the i_fps parameters in the driver configuration. Furthermore, utilizing a compressed transport or configuring the DDS implementation (e.g., CycloneDDS) to use shared memory (if the driver and Nav2 are on the same host) can significantly reduce latency. The depthai_ros_driver also supports on-device decimation (i_decimation_filter). A decimation factor of 2 reduces the point cloud size by 75% while retaining sufficient detail for obstacle avoidance, drastically lowering CPU usage on the Jetson/Pi host.24. Visual SLAM Architecture: Configuring RTAB-MapRTAB-Map (Real-Time Appearance-Based Mapping) serves as the cognitive core of this architecture. It replaces the traditional Gmapping or Cartographer nodes. The challenge in a LiDAR-less setup is configuring RTAB-Map to generate a 2D occupancy grid—required by Nav2—solely from the camera's 3D depth projection.4.1 The "Grid/FromDepth" Projection MechanismStandard 2D SLAM algorithms expect a sensor_msgs/LaserScan. When LiDAR is removed, we must synthesize this scan from volumetric data. RTAB-Map provides an internal mechanism for this via the Grid/FromDepth parameter (migrated to Grid/Sensor in recent ROS 2 releases).4Parameter Configuration for Depth-Only Mapping:ParameterValueDescriptionGrid/FromDepthtrueForces generation of occupancy grid from depth camera accumulation.Grid/RangeMax4.0Sets the effective range of the synthetic scan (meters).Grid/RayTracingtrueEnables clearing of empty space. Critical for dynamic environments.Grid/3DfalseProjects 3D voxels down to a 2D plane to save memory and CPU.When Grid/FromDepth is true (or Grid/Sensor is 1), RTAB-Map takes the incoming point cloud, projects it onto the ground plane (defined by the Reg/Force3DoF constraint), and performs ray-tracing. Ray-tracing is vital: it casts a vector from the camera center to each obstacle point. All grid cells traversed by this vector are marked as "free space." Without Grid/RayTracing: true, obstacles that move (or ghost noise) would remain permanently on the map, eventually trapping the robot.74.2 Constraining Degrees of Freedom (3DoF)Visual Odometry (VO) inherently estimates pose in 6 Degrees of Freedom ($x, y, z, roll, pitch, yaw$). However, ground vehicles like the UGV Beast operate on a 2D manifold. Allowing 6DoF estimation introduces vertical drift; a slight error in pitch estimation can cause the ground plane to appear as a "wall" in the map, causing navigation failure.To prevent this, the parameter Reg/Force3DoF must be set to true.5 This acts as a hard constraint on the graph optimization process, projecting all edges onto the $SE(2)$ plane ($x, y, \theta$). This ensures that the generated 2D map remains perfectly flat and aligned with the Nav2 costmaps.4.3 Sensor Fusion: Combining Wheel and Visual OdometryWhile RTAB-Map includes a visual odometry node (rgbd_odometry), utilizing it in isolation is risky. Rapid rotation or featureless walls can cause tracking loss. The UGV Beast's hardware architecture provides wheel odometry (via the slushe_driver_node described in Section 2.2), which is robust to texture loss but suffers from drift over distance.The optimal architecture uses Sensor Fusion. This can be achieved in two ways:EKF Fusion (Robot Localization): Using robot_localization to fuse /odom (wheel) and visual odometry output into a single filtered odometry topic.RTAB-Map Guessing: Configuring RTAB-Map to use wheel odometry as a "guess" for visual registration.The latter approach is often simpler and highly effective. By setting odom_topic to the wheel odometry and visual_odometry to true, RTAB-Map uses the wheel encoder delta to predict the camera's position in the next frame. This limits the search space for visual feature matching, increasing robustness and speed.10Launch Argument Configuration:Pythonlaunch_arguments = {
    'visual_odometry': 'true',
    'odom_topic': '/odom',          # Wheel odometry from UGV driver
    'odom_guess_frame_id': 'odom',  # Use wheel odom frame as guess
    'subscribe_scan': 'false',      # Disable LiDAR subscription
    'subscribe_depth': 'true',      # Enable Depth subscription
    'frame_id': 'base_link',
    'rtabmap_args': '--delete_db_on_start --Reg/Force3DoF true --Grid/FromDepth true'
}
4.4 Map Saving and Localization ModeRTAB-Map uses an SQLite database (~/.ros/rtabmap.db) to store the map. Unlike Gmapping, which produces a .pgm file at the end, RTAB-Map updates this database incrementally.Saving the Map:Upon terminating the ROS node, the database is saved automatically. To generate the standard map files (map.yaml, map.pgm) required by some legacy tools, one can use the map_saver_cli from Nav2, targeting the /map topic published by RTAB-Map.11Localization Mode:To switch from mapping to autonomous navigation:Relaunch the stack with localization:=true.Remove --delete_db_on_start from rtabmap_args.This sets Mem/IncrementalMemory to false, preventing the map from growing and forcing the system to localize against the existing database.4 Nav2 will then receive the static map from RTAB-Map's /map topic.5. Navigation 2 (Nav2) Stack AdaptationThe standard Nav2 configuration for the UGV Beast anticipates a LaserScan input. Removing the LiDAR breaks the obstacle_layer in the costmap parameters. The navigation stack must be reconfigured to ingest 3D PointClouds via the VoxelLayer.5.1 Configuring the Voxel LayerThe nav2_costmap_2d::VoxelLayer is designed to handle 3D data. Unlike the ObstacleLayer which marks pixels on a 2D grid, the VoxelLayer maintains a 3D voxel grid of the environment.13 This allows the robot to perceive obstacles that might be above or below a single laser scan plane (e.g., table overhangs).Local Costmap Configuration (nav2_params.yaml):YAMLlocal_costmap:
  local_costmap:
    ros__parameters:
      plugins: ["voxel_layer", "inflation_layer"] # Replaces obstacle_layer
      voxel_layer:
        plugin: "nav2_costmap_2d::VoxelLayer"
        enabled: True
        footprint_clearing_enabled: True
        max_obstacle_height: 2.0
        publish_voxel_map: True
        origin_z: 0.0
        z_resolution: 0.05
        z_voxels: 16
        unknown_threshold: 15
        mark_threshold: 0
        observation_sources: pointcloud
        pointcloud:
          topic: /oak/points
          max_obstacle_height: 2.0
          min_obstacle_height: 0.1  # Filters out floor noise
          obstacle_max_range: 3.0
          obstacle_min_range: 0.0
          raytrace_max_range: 3.5   # Must be > obstacle_max_range for clearing
          clearing: True
          marking: True
          data_type: "PointCloud2"
Parameter Analysis:z_voxels (16) * z_resolution (0.05) = 0.8m: This configuration creates a sensing volume 0.8 meters high. Any object within this volume will be projected down to the 2D costmap.14clearing: True: This parameter is critical for dynamic obstacle avoidance. It enables ray-tracing from the camera origin to the hit point. Voxels along this ray are cleared (set to free space). If a person walks away, this clears their previous position from the map.15min_obstacle_height: 0.1: This acts as a floor filter. Since the visual odometry might pitch slightly, the ground plane might occasionally intersect the voxel grid. Setting a minimum height prevents the robot from treating the floor as an obstacle.165.2 Spatio-Temporal Voxel Layer (STVL)An advanced alternative to the standard VoxelLayer is the Spatio-Temporal Voxel Layer (STVL). Depth cameras are inherently noisier than LiDAR, often producing transient "speckle" noise that appears as momentary obstacles. STVL introduces a temporal decay to the voxels.17 Voxels are not cleared instantly but decay over time. This acts as a low-pass filter for the occupancy grid, significantly smoothing the robot's path planning in noisy visual environments. If the compute resources on the Jetson Orin allow, substituting VoxelLayer with stvl_layer is recommended for smoother navigation.5.3 Global Costmap StrategyThe global costmap uses the static map provided by RTAB-Map for long-term planning. However, it should also include the voxel_layer configured identically to the local costmap. This allows the global planner (e.g., SmacPlanner) to plan paths that account for new obstacles that were not present when the map was originally recorded.156. Operational Workflows and IntegrationThe deployment of this architecture requires a unified launch sequence that orchestrates the hardware driver, perception, SLAM, and navigation nodes.6.1 The Master Launch File (bringup_rgbd.launch.py)A custom launch file is required to replace the manufacturer's bringup_lidar.launch.py. This file must execute the following sequence:URDF State Publisher: Loads the robot description. Crucial: The URDF must contain the oak_link and its transform relative to base_link. If the LiDAR link is present but unused, it is benign, but the camera transform must be accurate to prevent map distortion.Sub-Controller Driver: Launches the node communicating with the ESP32 (handling JSON comms and Heartbeat).OAK-D Driver: Launches depthai_ros_driver with the RGBD pipeline and IR projector enabled.RTAB-Map: Launches the SLAM node with Grid/FromDepth and Reg/Force3DoF.Nav2: Launches the navigation stack with the voxel-aware costmap configuration.6.2 Simulation and ValidationBefore deploying to the physical UGV Beast, validation in Gazebo is highly recommended. The ugv_gazebo package provided by Waveshare supports simulation.1 The simulation launch file must be modified to mimic the sensor removal:Disable the LiDAR plugin in the Gazebo URDF.Ensure the simulated depth camera plugin is active.Run the exact same Nav2 and RTAB-Map launch configurations against the simulated robot.This isolates logical errors in the TF tree or costmap configuration from physical hardware issues like camera calibration or lighting.6.3 High-Level Semantic Planning ("Sasquatch" Architecture)While the scope of this report focuses on navigation mechanics, the transition to a vision-centric stack opens the door to higher-level cognition, referenced in the "Sasquatch" architecture documents.1 Unlike LiDAR, which provides only geometry, the OAK-D stream contains semantic information (color, texture).Future integration phases can leverage this by feeding the RGB stream into a Visual Language Model (V-LLM) or a semantic segmentation network. This would allow the robot to navigate not just to coordinates (x, y), but to semantic targets (e.g., "Navigate to the sink," "Avoid the wet floor sign"). The depthai_ros_driver supports running neural networks directly on the camera (e.g., MobileNet or YOLO), publishing detection bounding boxes that can be integrated into the costmap as "semantic layers," marking specific object classes as lethal obstacles or preferred zones.207. Conclusion and RecommendationsReconfiguring the UGV Beast for RGB-D only autonomy is a rigorous engineering challenge that trades the geometric simplicity of LiDAR for the rich, volumetric potential of active stereo vision. The success of this transition rests on three pillars:Active Perception: The rigorous configuration of the OAK-D Pro's IR dot projector (i_laser_dot_brightness: 800) is non-negotiable for indoor operation. Without it, the system will fail in low-texture areas.Geometric Constraint: Forcing RTAB-Map to assume a 2D world (Reg/Force3DoF) and synthesizing a scan from depth (Grid/FromDepth) bridges the gap between the 3D sensor and the 2D navigation stack.Volumetric Navigation: Adapting Nav2 to use VoxelLayer allows the robot to navigate safely in a 3D world, detecting obstacles that a 2D laser would miss.By adhering to the parameter sets and architectural guidelines presented in this report, the UGV Beast can achieve a level of autonomy that is not only independent of LiDAR but richer in environmental awareness, paving the way for advanced semantic robotics applications.8. Appendix: Summary of Critical Configuration ParametersOAK-D Driver (camera.yaml):i_pipeline_type: RGBDi_enable_ir: truei_laser_dot_brightness: 800i_align_depth: trueRTAB-Map (rtabmap.launch.py):subscribe_scan: falsesubscribe_depth: trueGrid/FromDepth: trueReg/Force3DoF: trueodom_guess_frame_id: odom (Wheel Odometry)Nav2 (nav2_params.yaml):plugins: ["voxel_layer", "inflation_layer"]voxel_layer/observation_sources: pointcloudvoxel_layer/clearing: truevoxel_layer/z_voxels: 16 (for 0.8m height)
